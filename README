--------------------------------------------Overview---------------------------------------------------------------

This project implements an end-to-end data pipeline for collecting, processing, storing, and analyzing news articles.
The goal is to extract insights, named entities, and sentiment analysis from real-world news and make them explorable 
via Kibana dashboards.
![Architecture Diagram](architecture.png)
--------------------------------------------Architecture---------------------------------------------------------------

-> Data Flow
1-Data Ingestion
   Fetch news articles from external News API sources.
   Stream data in real-time using Apache Kafka.

2-Data Storage
   Store raw data into HDFS for durability and scalability.

3-Data Processing & Analysis
   Use Apache Spark to process news articles:
   Data cleaning & transformation
   Named Entity Recognition (NER) (ORG, GPE, PERSON, etc.)
   Sentiment Analysis of the news text
   Store processed data into MongoDB for flexible querying.

4-Indexing & Search
   Push enriched news articles into Elasticsearch.

5-Visualization
   Explore, search, and analyze data via Kibana dashboards.

->Tech Stack
Ingestion: News API, Apache Kafka
Storage: HDFS, MongoDB
Processing: Apache Spark (NER, sentiment analysis)
Indexing/Search: Elasticsearch
Visualization: Kibana
Orchestration: Docker / Docker Compose

--------------------------------------------How it works---------------------------------------------------------------

Collect News → API → Kafka topic

Persist Raw Data → Kafka consumer → HDFS

Process Data → Spark job (NER + Sentiment)

Save Results → MongoDB (structured articles)

Index in Elasticsearch → For fast searching

Visualize → Kibana dashboards
