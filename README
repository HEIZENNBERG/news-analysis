--------------------------------------------Overview---------------------------------------------------------------

This project implements an end-to-end data pipeline for collecting, processing, storing, and analyzing news articles.
The goal is to extract insights, named entities, and sentiment analysis from real-world news and make them explorable 
via Kibana dashboards.

--------------------------------------------Architecture---------------------------------------------------------------

-> Data Flow
1-Data Ingestion
   Fetch news articles from external News API sources.
   Stream data in real-time using Apache Kafka.

2-Data Storage
   Store raw data into HDFS for durability and scalability.

3-Data Processing & Analysis
   Use Apache Spark to process news articles:
   Data cleaning & transformation
   Named Entity Recognition (NER) (ORG, GPE, PERSON, etc.)
   Sentiment Analysis of the news text
   Store processed data into MongoDB for flexible querying.

4-Indexing & Search
   Push enriched news articles into Elasticsearch.

5-Visualization
   Explore, search, and analyze data via Kibana dashboards.

->Tech Stack
Ingestion: News API, Apache Kafka
Storage: HDFS, MongoDB
Processing: Apache Spark (NER, sentiment analysis)
Indexing/Search: Elasticsearch
Visualization: Kibana
Orchestration: Docker / Docker Compose


 How It Works
```mermaid
flowchart TD
    A[Collect News] --> B[API]
    B --> C[Kafka Topic]
    C --> D[Kafka Consumer]
    D --> E[HDFS]

    E --> F[Spark Job: NER + Sentiment]
    F --> G[MongoDB: Structured Articles]
    G --> H[Elasticsearch: Fast Search]
    H --> I[Kibana: Visualization]

Visualize â†’ Kibana dashboards
